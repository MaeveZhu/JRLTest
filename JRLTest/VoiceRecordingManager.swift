import Foundation
import Speech
import AVFoundation
import Intents
import CoreLocation

class VoiceRecordingManager: NSObject, ObservableObject {
    static let shared = VoiceRecordingManager()
    
    @Published var isListening = false
    @Published var isRecording = false
    @Published var currentTestSession: TestSession?
    @Published var recordingSegments: [RecordingSegment] = []
    
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine = AVAudioEngine()
    
    private var audioRecorder: AVAudioRecorder?
    private var silenceTimer: Timer?
    private var recordingStartTime: Date?
    private var currentSegmentNumber = 1
    
    // Ê∑ªÂä†Áä∂ÊÄÅÁÆ°ÁêÜ
    private var isEngineRunning = false
    private var hasTapInstalled = false
    
    // Ëß¶ÂèëÂÖ≥ÈîÆËØç
    private let triggerKeywords = ["ÂºÄÂßãËÆ∞ÂΩï", "ÂºÄÂßãÂΩïÈü≥", "ËÆ∞ÂΩïÂºÄÂßã", "start recording", "hey siri", "Âòøsiri"]
    
    override init() {
        super.init()
        setupSpeechRecognizer()
        requestPermissions()
    }
    
    // MARK: - Setup
    private func setupSpeechRecognizer() {
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
        speechRecognizer?.delegate = self
    }
    
    private func requestPermissions() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                case .denied, .restricted, .notDetermined:
                    print("‚ùå Speech recognition not authorized")
                @unknown default:
                    print("‚ùå Unknown speech recognition status")
                }
            }
        }
    }
    
    // MARK: - Test Session Management
    func startTestSession(vin: String, testExecutionId: String, tag: String, startCoordinate: CLLocationCoordinate2D?) {
        currentTestSession = TestSession(
            vin: vin,
            testExecutionId: testExecutionId,
            tag: tag,
            startCoordinate: startCoordinate,
            startTime: Date()
        )
        recordingSegments = []
        currentSegmentNumber = 1
        
        // Âª∂ËøüÂêØÂä®ÈÅøÂÖçUIÂÜ≤Á™Å
        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
            self.startListening()
        }
        
        print("‚úÖ Test session started for VIN: \(vin)")
    }
    
    func endTestSession(endCoordinate: CLLocationCoordinate2D?) -> TestSession? {
        guard var session = currentTestSession else { return nil }
        
        // ÂÆåÂÖ®ÂÅúÊ≠¢ÊâÄÊúâÈü≥È¢ëÊìç‰Ωú
        completelyStopAudio()
        
        session.endCoordinate = endCoordinate
        session.endTime = Date()
        session.recordingSegments = recordingSegments
        
        saveTestSession(session)
        
        // ÈáçÁΩÆÁä∂ÊÄÅ
        currentTestSession = nil
        recordingSegments = []
        currentSegmentNumber = 1
        
        print("‚úÖ Test session ended for VIN: \(session.vin)")
        return session
    }
    
    // MARK: - Audio Management
    private func completelyStopAudio() {
        print("üõë Completely stopping all audio operations")
        
        // ÂÅúÊ≠¢ÂΩïÈü≥
        if isRecording {
            audioRecorder?.stop()
            audioRecorder = nil
            isRecording = false
        }
        
        // ÂÅúÊ≠¢ËØ≠Èü≥ËØÜÂà´
        recognitionTask?.cancel()
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        recognitionTask = nil
        
        // ÂÅúÊ≠¢Èü≥È¢ëÂºïÊìé
        if isEngineRunning {
            audioEngine.stop()
            isEngineRunning = false
        }
        
        // ÁßªÈô§tap
        if hasTapInstalled {
            audioEngine.inputNode.removeTap(onBus: 0)
            hasTapInstalled = false
        }
        
        // ÂÅúÊ≠¢ËÆ°Êó∂Âô®
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        isListening = false
        
        // ÈáçÁΩÆÈü≥È¢ë‰ºöËØù
        do {
            try AVAudioSession.sharedInstance().setActive(false)
        } catch {
            print("‚ö†Ô∏è Error deactivating audio session: \(error)")
        }
        
        print("‚úÖ All audio operations stopped")
    }
    
    // MARK: - Voice Recognition
    func startListening() {
        guard !isListening, currentTestSession != nil else { 
            print("‚ùå Cannot start listening: already listening or no session")
            return 
        }
        
        print("üé§ Starting voice listening...")
        
        // Á°Æ‰øùÂÆåÂÖ®Ê∏ÖÁêÜ‰πãÂâçÁöÑÁä∂ÊÄÅ
        completelyStopAudio()
        
        // Á≠âÂæÖ‰∏Ä‰∏ãÂÜçÂêØÂä®ÔºåÁ°Æ‰øùÊ∏ÖÁêÜÂÆåÊàê
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.actuallyStartListening()
        }
    }
    
    private func actuallyStartListening() {
        do {
            // 1. ÈÖçÁΩÆÈü≥È¢ë‰ºöËØù
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.record, mode: .measurement, options: [])
            try audioSession.setActive(true)
            
            // 2. ÂàõÂª∫Êñ∞ÁöÑËØÜÂà´ËØ∑Ê±Ç
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else {
                print("‚ùå Cannot create recognition request")
                return
            }
            recognitionRequest.shouldReportPartialResults = true
            
            // 3. ÈáçÊñ∞ÂàõÂª∫Èü≥È¢ëÂºïÊìéÔºàÁ°Æ‰øùÂπ≤ÂáÄÁä∂ÊÄÅÔºâ
            audioEngine = AVAudioEngine()
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            // 4. ÂÆâË£Ötap
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                recognitionRequest.append(buffer)
            }
            hasTapInstalled = true
            
            // 5. ÂêØÂä®Èü≥È¢ëÂºïÊìé
            audioEngine.prepare()
            try audioEngine.start()
            isEngineRunning = true
            
            // 6. ÂºÄÂßãËØÜÂà´‰ªªÂä°
            recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                DispatchQueue.main.async {
                    if let result = result {
                        let spokenText = result.bestTranscription.formattedString
                        self?.processSpokenText(spokenText)
                    }
                    
                    if let error = error {
                        print("‚ùå Recognition error: \(error)")
                        // ÈáçËØïÊú∫Âà∂
                        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
                            if self?.currentTestSession != nil && self?.isListening == true {
                                self?.startListening()
                            }
                        }
                    }
                }
            }
            
            isListening = true
            print("‚úÖ Voice listening started successfully")
            
        } catch {
            print("‚ùå Failed to start listening: \(error)")
            completelyStopAudio()
            
            // ÈáçËØï
            DispatchQueue.main.asyncAfter(deadline: .now() + 3.0) {
                if self.currentTestSession != nil {
                    self.startListening()
                }
            }
        }
    }
    
    func stopListening() {
        print("üõë Stopping voice listening...")
        completelyStopAudio()
    }
    
    private func processSpokenText(_ text: String) {
        print("üé§ Heard: \(text)")
        
        let normalizedText = text.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
        
        // Ê£ÄÊü•Ëß¶ÂèëËØç
        for keyword in triggerKeywords {
            if normalizedText.contains(keyword.lowercased()) {
                print("‚úÖ Trigger word detected: \(keyword)")
                startVoiceTriggeredRecording()
                return
            }
        }
    }
    
    // MARK: - Recording Management
    private func startVoiceTriggeredRecording() {
        guard !isRecording, let session = currentTestSession else {
            print("‚ùå Cannot start recording: already recording or no session")
            return
        }
        
        print("üéôÔ∏è Starting voice-triggered recording...")
        
        // ÂÆåÂÖ®ÂÅúÊ≠¢ËØ≠Èü≥ÁõëÂê¨
        completelyStopAudio()
        
        // Á≠âÂæÖ‰∏Ä‰∏ãÂÜçÂºÄÂßãÂΩïÈü≥
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.actuallyStartRecording()
        }
    }
    
    private func actuallyStartRecording() {
        guard let session = currentTestSession else { return }
        
        do {
            // 1. ÈÖçÁΩÆÈü≥È¢ë‰ºöËØù‰∏∫ÂΩïÈü≥
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker])
            try audioSession.setActive(true)
            
            // 2. ÂàõÂª∫ÂΩïÈü≥Êñá‰ª∂
            let fileName = "segment_\(currentSegmentNumber)_\(Date().timeIntervalSince1970).m4a"
            let recordingsURL = try createRecordingsDirectory()
            let fileURL = recordingsURL.appendingPathComponent(fileName)
            
            // 3. ÂΩïÈü≥ËÆæÁΩÆ
            let settings: [String: Any] = [
                AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
                AVSampleRateKey: 44100.0,
                AVNumberOfChannelsKey: 1,
                AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
            ]
            
            // 4. ÂàõÂª∫ÂΩïÈü≥Âô®
            audioRecorder = try AVAudioRecorder(url: fileURL, settings: settings)
            audioRecorder?.delegate = self
            audioRecorder?.record()
            
            recordingStartTime = Date()
            isRecording = true
            
            // 5. ÂºÄÂßãÈùôÈü≥Ê£ÄÊµã
            startSilenceDetection()
            
            print("‚úÖ Recording started: \(fileName)")
            
        } catch {
            print("‚ùå Recording failed: \(error)")
            
            // ÂΩïÈü≥Â§±Ë¥•ÔºåÈáçÊñ∞ÂºÄÂßãÁõëÂê¨
            DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                if self.currentTestSession != nil {
                    self.startListening()
                }
            }
        }
    }
    
    private func stopCurrentRecording() {
        guard isRecording, let recorder = audioRecorder else { return }
        
        print("üõë Stopping current recording...")
        
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        recorder.stop()
        
        // ÂàõÂª∫ÂΩïÈü≥ÁâáÊÆµ
        if let startTime = recordingStartTime,
           let session = currentTestSession {
            let segment = RecordingSegment(
                id: UUID(),
                segmentNumber: currentSegmentNumber,
                fileName: recorder.url.lastPathComponent,
                fileURL: recorder.url,
                startTime: startTime,
                endTime: Date(),
                vin: session.vin,
                testExecutionId: session.testExecutionId,
                tag: session.tag
            )
            
            recordingSegments.append(segment)
            currentSegmentNumber += 1
            
            print("‚úÖ Recording segment \(segment.segmentNumber) saved")
        }
        
        audioRecorder = nil
        recordingStartTime = nil
        isRecording = false
        
        // ÂΩïÈü≥ÁªìÊùüÂêéÔºåÈáçÊñ∞ÂºÄÂßãÁõëÂê¨
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.5) {
            if self.currentTestSession != nil {
                self.startListening()
            }
        }
    }
    
    private func startSilenceDetection() {
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 3.0, repeats: false) { [weak self] _ in
            print("üîá 3-second silence detected")
            DispatchQueue.main.async {
                self?.stopCurrentRecording()
            }
        }
    }
    
    // MARK: - File Management
    private func createRecordingsDirectory() throws -> URL {
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let recordingsURL = documentsPath.appendingPathComponent("VoiceRecordings")
        
        if !FileManager.default.fileExists(atPath: recordingsURL.path) {
            try FileManager.default.createDirectory(at: recordingsURL, withIntermediateDirectories: true)
        }
        
        return recordingsURL
    }
    
    // MARK: - Data Persistence
    private func saveTestSession(_ session: TestSession) {
        var sessions = getTestSessions()
        sessions.append(session)
        
        if let data = try? JSONEncoder().encode(sessions) {
            UserDefaults.standard.set(data, forKey: "TestSessions")
        }
    }
    
    func getTestSessions() -> [TestSession] {
        guard let data = UserDefaults.standard.data(forKey: "TestSessions"),
              let sessions = try? JSONDecoder().decode([TestSession].self, from: data) else {
            return []
        }
        return sessions
    }
}

// MARK: - Delegates
extension VoiceRecordingManager: SFSpeechRecognizerDelegate {
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        print("Speech recognizer availability: \(available)")
    }
}

extension VoiceRecordingManager: AVAudioRecorderDelegate {
    func audioRecorderDidFinishRecording(_ recorder: AVAudioRecorder, successfully flag: Bool) {
        print("Recording finished successfully: \(flag)")
    }
}

// ‰øùÊåÅÊï∞ÊçÆÊ®°Âûã‰∏çÂèò...
struct TestSession: Identifiable, Codable {
    let id = UUID()
    let vin: String
    let testExecutionId: String
    let tag: String
    let startCoordinate: CLLocationCoordinate2D?
    var endCoordinate: CLLocationCoordinate2D?
    let startTime: Date
    var endTime: Date?
    var recordingSegments: [RecordingSegment] = []
}

struct RecordingSegment: Identifiable, Codable {
    let id: UUID
    let segmentNumber: Int
    let fileName: String
    let fileURL: URL
    let startTime: Date
    let endTime: Date
    let vin: String
    let testExecutionId: String
    let tag: String
    
    var duration: TimeInterval {
        return endTime.timeIntervalSince(startTime)
    }
    
    var formattedDuration: String {
        let minutes = Int(duration) / 60
        let seconds = Int(duration) % 60
        return String(format: "%02d:%02d", minutes, seconds)
    }
    
    var formattedTime: String {
        let formatter = DateFormatter()
        formatter.timeStyle = .short
        return formatter.string(from: startTime)
    }
} 